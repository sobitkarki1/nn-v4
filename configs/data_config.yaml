dataset:
  name: "the_pile"  # Dataset name
  path: "data/the_pile"  # Local path or HuggingFace dataset name
  
  # Streaming mode (for large datasets)
  streaming: true
  
  # Splits
  train_split: "train"
  val_split: "validation"
  
  # Preprocessing
  max_length: 2048  # Sequence length
  
  # Tokenizer
  tokenizer_type: "gpt2"  # gpt2, custom, or path to tokenizer
  # This tokenizer has vocab size of 50k which is fine for now

  tokenizer_path: null  # Path to custom tokenizer if needed
  
  # Cache
  cache_dir: "data/cache"
  
  # Data loading
  shuffle_buffer_size: 10000  # For streaming datasets
  
# Alternative: Use HuggingFace datasets directly
# dataset:
#   name: "EleutherAI/pile"
#   path: "EleutherAI/pile"
#   streaming: true
