model:
  vocab_size: 50257
  n_layer: 12  # ~100M parameters
  n_head: 12
  n_embd: 768
  context_length: 1024
  dropout: 0.0
  bias: false
  
  # Positional encoding
  use_rope: true  # Rotary Position Embeddings (RoPE)
  rope_theta: 10000.0
  
  # Attention
  use_flash_attn: false  # Disabled for testing
  
  # Normalization
  layer_norm_epsilon: 0.00001
  
  # Feed-forward
  ffn_hidden_mult: 4  # FFN hidden size = n_embd * 4
